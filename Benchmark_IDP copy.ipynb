{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark des solutions d'extraction de documents\n",
    "Ce notebook compare trois solutions d'extraction de contenu de documents :\n",
    "- **Docling v2**\n",
    "- **LayoutParser**\n",
    "- **Tesseract + TableBank**\n",
    "\n",
    "Les métriques évaluées sont :\n",
    "- **Temps de traitement**\n",
    "- **Similarité textuelle** (par rapport à un texte de référence)\n",
    "\n",
    "Une interface permet de visualiser les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import getpass, subprocess, platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer Tesseract OCR sur Linux\n",
    "\n",
    "def run_sudo(cmd):\n",
    "    subprocess.run(f\"echo {getpass.getpass('Mot de passe sudo : ')} | sudo -S {cmd}\", shell=True)\n",
    "\n",
    "run_sudo(\"apt update -qq && apt install -y tesseract-ocr -qq\")\n",
    "subprocess.run(\"tesseract --version\", shell=True, check=True)\n",
    "print(f\"\\nDétails de l'OS: {platform.platform()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de pytesseract, docling, layoutparser, kaggle, datasets, panel, ipywidgets via pip\n",
    "libs = {\n",
    "    \"Solution Benchmarkées\": [\"pytesseract\", \"docling\", \"layoutparser\", \"paddleocr\", \"paddlepaddle\"],\n",
    "    \"Données\": [\"kaggle\", \"datasets\"],\n",
    "    \"Évaluation & metrics\": [\"evaluate\", \"jiwer\", \"rouge-score\"],\n",
    "    \"UI\": [\"panel[recommended]\", \"ipywidgets\"]\n",
    "}\n",
    "\n",
    "print(\"Début de l'installation des dépendances...\")\n",
    "\n",
    "# Installation avec barre de progression\n",
    "for category, packages in libs.items():\n",
    "    for lib in tqdm(packages, desc=f\"Installation: {category}\"):\n",
    "        subprocess.run(f\"pip install -q {lib}\", shell=True)\n",
    "\n",
    "print(\"Installation terminée.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dossier contenant les fichiers texte\n",
    "TEXT_FILES_DIR = '/home/pi/Téléchargements/data-gouv-pdf-txt/data_gouv_txt/'\n",
    "# Dossier où les PDF seront téléchargés\n",
    "DOWNLOAD_DIR = '/home/pi/Téléchargements/data-gouv-pdf-txt/data_gouv_pdf/'\n",
    "# Fichier pour enregistrer les erreurs\n",
    "ERROR_LOG = '/home/pi/Téléchargements/data-gouv-pdf-txt/errors.log'\n",
    "# Délai entre les requêtes pour éviter de surcharger le serveur (en secondes)\n",
    "REQUEST_DELAY = 2\n",
    "\n",
    "def extract_ids(filename):\n",
    "    \"\"\"Extrait l'ID du dataset et l'ID de la ressource à partir du nom du fichier.\"\"\"\n",
    "    base_name = os.path.basename(filename)\n",
    "    dataset_id, resource_id = base_name.replace('.txt', '').split('--')\n",
    "    return dataset_id, resource_id\n",
    "\n",
    "def get_resource_url(dataset_id, resource_id):\n",
    "    \"\"\"Récupère l'URL de la ressource à partir de l'API de data.gouv.fr.\"\"\"\n",
    "    api_url = f'https://www.data.gouv.fr/api/1/datasets/{dataset_id}/'\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        dataset = response.json()\n",
    "        for resource in dataset.get('resources', []):\n",
    "            if resource['id'] == resource_id:\n",
    "                return resource.get('url')\n",
    "    else:\n",
    "        print(f\"Erreur lors de l'accès à {api_url} : {response.status_code}\")\n",
    "    return None\n",
    "\n",
    "def download_pdf(pdf_url, download_path):\n",
    "    \"\"\"Télécharge le PDF depuis l'URL spécifiée vers le chemin de téléchargement.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(download_path, 'wb') as pdf_file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        pdf_file.write(chunk)\n",
    "            print(f\"Téléchargé : {download_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Erreur lors du téléchargement de {pdf_url} : {response.status_code}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Erreur réseau lors du téléchargement de {pdf_url} : {e}\")\n",
    "    return False\n",
    "\n",
    "def get_all_text_files(directory):\n",
    "    \"\"\"Récupère tous les fichiers texte dans le répertoire et ses sous-répertoires.\"\"\"\n",
    "    text_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                text_files.append(os.path.join(root, file))\n",
    "    return text_files\n",
    "\n",
    "def log_error(dataset_id, resource_id):\n",
    "    \"\"\"Enregistre une erreur dans le fichier d'erreurs.\"\"\"\n",
    "    with open(ERROR_LOG, 'a') as log_file:\n",
    "        log_file.write(f\"{dataset_id}--{resource_id}\\n\")\n",
    "\n",
    "def load_errors():\n",
    "    \"\"\"Charge les erreurs précédemment enregistrées.\"\"\"\n",
    "    if not os.path.exists(ERROR_LOG):\n",
    "        return []\n",
    "    with open(ERROR_LOG, 'r') as log_file:\n",
    "        return [line.strip() for line in log_file.readlines()]\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(DOWNLOAD_DIR):\n",
    "        os.makedirs(DOWNLOAD_DIR)\n",
    "\n",
    "    # Charger les fichiers texte et les erreurs précédentes\n",
    "    text_files = get_all_text_files(TEXT_FILES_DIR)\n",
    "    errors = load_errors()\n",
    "\n",
    "    if not text_files:\n",
    "        print(\"Aucun fichier texte trouvé dans le dossier spécifié.\")\n",
    "        return\n",
    "\n",
    "    for text_file in tqdm(text_files, desc=\"Traitement des fichiers texte\"):\n",
    "        dataset_id, resource_id = extract_ids(text_file)\n",
    "        pdf_filename = f\"{dataset_id}--{resource_id}.pdf\"\n",
    "        download_path = os.path.join(DOWNLOAD_DIR, pdf_filename)\n",
    "\n",
    "        # Ne pas retélécharger si le fichier existe déjà\n",
    "        if os.path.exists(download_path):\n",
    "            print(f\"Déjà téléchargé : {download_path}\")\n",
    "            continue\n",
    "\n",
    "        # Vérifier si cette ressource est déjà dans les erreurs\n",
    "        if f\"{dataset_id}--{resource_id}\" in errors:\n",
    "            print(f\"Échec précédent pour : {dataset_id}--{resource_id}\")\n",
    "            continue\n",
    "\n",
    "        pdf_url = get_resource_url(dataset_id, resource_id)\n",
    "        if pdf_url:\n",
    "            success = download_pdf(pdf_url, download_path)\n",
    "            if not success:\n",
    "                log_error(dataset_id, resource_id)\n",
    "        else:\n",
    "            print(f\"URL PDF non trouvée pour la ressource {resource_id} dans le dataset {dataset_id}\")\n",
    "            log_error(dataset_id, resource_id)\n",
    "\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--20f7b628-3300-420a-8e3d-90144397d176.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--9b60cd53-6fa6-48c2-8e69-4a6217483b87.pdf\n",
      "Corresponding TXT file not found for 5b4eeb71a3a7297183e1c36f--8efb868f-0e82-4e5d-af57-5b6a8c109887.pdf\n",
      "Corresponding TXT file not found for 5369998ea3a729239d2052a1--4d9ffbbe-f55d-428a-9e53-ebcc1bc835bd.pdf\n",
      "Corresponding TXT file not found for 5cfa38578b4c411846c84b99--002a5bec-805e-4334-8ddb-b06411779883.pdf\n",
      "Corresponding TXT file not found for 5b4ee8a7b595087918d496ca--d7238360-0ae4-4e80-8b27-c0f7000f7f78.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--2f5b5b28-ff36-4ea0-82fc-e46eef042f72.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--9c8e7b03-4ba6-4e77-b5a9-67e341b7b433.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--6cd6e1aa-074d-48c5-952b-1ec87c377ef5.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--50547a27-80a1-4878-9cc1-d9c8f144218a.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--b3b4f16c-7de6-4a79-8aa5-2fa3ac249433.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--8b6f2dbf-5669-4223-ba18-6f14161e1b83.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--20358987-e5c1-47db-b4b5-d7868974f2ac.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--7ec667a9-42ae-44da-bdbf-086c30f9cdd9.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--39ef7411-b61e-4a0c-a923-03b8b7c03366.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--de0bbe74-2e2c-4b7c-be4d-8f2e28465ea6.pdf\n",
      "Corresponding TXT file not found for 541010f3a3a72937cb2c7039--5ff1086a-6a67-45f6-aef5-bcae9a76331e.pdf\n",
      "Corresponding TXT file not found for 5369a25aa3a729239d20680d--ee98e853-8c6c-4946-9699-396046f98325.pdf\n",
      "Corresponding TXT file not found for 5b10f5e2c751df2c87b90aae--3a2afb7c-eceb-471b-8bda-fc6530458cc9.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--3381e2f8-6e80-4f08-acee-0f47d727697b.pdf\n",
      "Corresponding TXT file not found for 5b4ee7e7b5950877c5d496cd--ae6ccee7-4a47-4ac2-8a19-335c51227972.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--ff0afa00-73a0-4841-b8df-95eb52435682.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--bf4bfe41-0ef2-4c3c-9a8f-60b28e5475d8.pdf\n",
      "Corresponding TXT file not found for 5cfa35a28b4c411454807639--04332add-7466-455b-b79b-11cee69ebaf7.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--b6477aca-c1a4-4d01-a63b-348f9b499536.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--d2d2fffc-3390-462f-9491-460030fa88e5.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--98d19b35-efb6-40e5-ac4a-479ed356c9c5.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--887dc592-b918-44a3-9a01-9a2b2b05381b.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--42b287a2-d108-41c2-8be2-eca1075d7c13.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--9adb2404-170d-45cb-bb16-75b22aa61408.pdf\n",
      "Corresponding TXT file not found for 5b6952a88b4c4119b83b67f7--e2d27ad9-2538-428b-9856-5f77a87548d9.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--785bbfe6-a60d-4165-a56f-564af49af7d5.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--79476901-a500-4ab4-9224-4a1f00a89bc6.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--6c72ddaf-c5a0-4267-9e9a-2d12381df9cb.pdf\n",
      "Corresponding TXT file not found for 5e4e59926f44415dca5a2b5b--48603ff4-b050-40a9-b96d-f1d68a2921d5.pdf\n",
      "Corresponding TXT file not found for 5d6f01d906e3e71f6bd78afd--2e9156b3-665e-45a2-b69c-e15d40088742.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--fdd6d535-1e12-4693-86be-47325135fbf5.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--adae961d-f44a-4f9c-9d76-ac0ccb0eb166.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--ca3ea704-f319-4f63-b6c0-9f0df573c9ed.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--cab0c4c3-6c5e-4a73-a102-b52e7d866aac.pdf\n",
      "Corresponding TXT file not found for 5cfa39a78b4c411846c84b9a--a092b8d3-607f-4603-a713-717c3eedb365.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--ff9839f0-a016-44c4-848a-f8d68b05be76.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--c1d46b23-2144-4498-976e-c6efe1dcb861.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--5e58b313-f86e-4421-8ddd-cbc65cf1bbe1.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--bc57e246-6051-416c-a2fe-43a988ab1c3c.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--b771ce32-e4bf-4e46-a5fc-0ce59acb2d78.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--fc10c062-f301-4e55-ac0e-0abe7404f352.pdf\n",
      "Corresponding TXT file not found for 5b4eeb62a3a7297183e1c36c--a2f2b572-5201-41d2-920f-291152d024b8.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--0cbd21a3-c1a9-44d2-a164-14808ef0b339.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--2e5493b3-1a04-4fd1-bba1-0a175a26b8f4.pdf\n",
      "Corresponding TXT file not found for 5cc0fdd406e3e72dd56fc4f9--7b9779ff-14bb-4a03-b444-c33cb2077c1d.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--64a32b2e-0ed5-4c4f-8f38-19d42655f4f9.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--77a77fb5-5657-4b14-b879-5b751bcf3c49.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--c0fa2694-6600-4911-98aa-214b36431e19.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--c39bc1ce-ce16-4415-9105-1573ca5a45ee.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--400ff62a-2f11-4162-9daf-ab928760bc6b.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--1d691fa2-71db-42c2-94d2-ba57f476d553.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--6f2c82e8-cc23-4ac7-bb38-9cc432f7d829.pdf\n",
      "Corresponding TXT file not found for 5b6952a88b4c4119b83b67f7--9264f434-f6c0-474f-828b-b3d6fdb0bdc5.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--d4976e27-7c8e-4661-bb9c-0ffdcd81190a.pdf\n",
      "Corresponding TXT file not found for 54101a78a3a72937cb2c7044--f382f1ce-4e37-4f30-b339-be9ff82daf33.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--bb1b2361-44be-434c-b5d2-fa91eafc60a1.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--c0b45e0b-ff61-40e1-aeaf-3040a894a9ac.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--6e037ccb-e250-4754-aba1-b2c8178ef32f.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--2ad326ca-00df-428a-92e8-48ee08684578.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--4356399b-7ade-4a87-b8ea-a8be4cef0d4a.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--f9a71734-5430-40c0-9466-15321f5fe50a.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--4d0427c8-31f1-4078-a8a8-ecbab2b78ae7.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--8a8cdd3e-33ba-4248-9728-80ef32effe23.pdf\n",
      "Corresponding TXT file not found for 59c50320c751df79c0e4dc73--e738c34c-e9eb-451f-b614-c12eb1c67cca.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--8faafa6e-655d-4308-881c-e3fc4ccd8b1a.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--55a4f1cd-0305-4f7b-abef-a58796b4d1a8.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--bd7902ad-7ac5-4c42-96c4-b094ca3959e3.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--4d56e075-8906-46e4-bcb1-86c601566830.pdf\n",
      "Corresponding TXT file not found for 5b71a9ea8b4c414c31b9a5bc--0bd16969-e01a-4d9d-b64b-4d1672041975.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--a54727db-0030-4849-9f9b-bd05dfc58858.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--8a7f955b-cb2a-43a1-b19e-90ed5e213457.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--c9182a9c-3744-4d91-8d8f-b91afeba2180.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--4083946f-e42b-451a-a2fc-81a61d305c80.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--21eca6e8-aa19-43b5-a379-2357a5b63bb1.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--121952e1-c4a2-409e-9176-511550abf370.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--d3620965-a53d-4228-8f00-0aed5359a2f9.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--b53e9d1c-71bd-4290-b6f7-db545c613952.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--64697a17-2fd9-4f54-9a14-0757aff0d3e1.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--e6c01e86-4ed4-405a-a5d2-a59f64e0fbab.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--e527f310-e820-470e-ac60-b19e0b20863f.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--55405e40-469b-4003-8396-07809e4d5ada.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--3f48339a-4daf-4a0b-93fc-6261227d5e3f.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--df7e539c-8daf-4065-bf8a-60ec0199580d.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--7c5d8e92-ff76-45e5-b6fa-083d38957f9c.pdf\n",
      "Corresponding TXT file not found for 5b71a9ea8b4c414c31b9a5bc--dc247d49-46fa-4ded-8a33-b86aa8fcdf5b.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--9d32b9dc-ffdc-4f54-b11e-0a2beed16dd8.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--17e0cc19-04d4-48e2-ab75-9d346452ebd0.pdf\n",
      "Corresponding TXT file not found for 5ce2bd1d06e3e707af260ccc--de12c3df-a3b3-49e3-b922-39cb5f74dd9e.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--bf87d9f6-66c2-4f22-81ea-f02693bbff7a.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--3b4401b6-37a8-401c-94c5-b3f0d0ddb228.pdf\n",
      "Corresponding TXT file not found for 53698e2aa3a729239d2033c3--d27156b1-0426-472f-b7b1-d17495603f5f.pdf\n",
      "Corresponding TXT file not found for 5a1dd4e5c751df02b116a0d6--c6284be1-296e-43e6-b7ae-58292f3f1fa3.pdf\n",
      "Corresponding TXT file not found for 53698e17a3a729239d203393--c17da0b6-f3e2-4cc8-a5ef-b88fad6fe3ad.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--8de84068-8dfc-4aaa-8435-35c265929263.pdf\n",
      "Corresponding TXT file not found for 5bbcbf488b4c41041b850cf7--7751bc70-01e8-4b88-8743-bafbada5475b.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--81315f15-67eb-4252-bc2d-aaa7e9ebd45d.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--84aacaf4-3072-4fe7-a6e2-e8a7c79baa0c.pdf\n",
      "Corresponding TXT file not found for 5cc0fdd406e3e72dd56fc4f9--6b4bfdd3-efbc-4035-9c7c-62b622bcf537.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--7151a7ba-4b40-4a11-b556-9fcb5af31874.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--c06bba34-b752-4dad-9f60-056e98abf215.pdf\n",
      "Corresponding TXT file not found for 5b4ee890a3a7296dbbe1c35b--f9aa2c38-bc99-4f29-a393-90461d285d4c.pdf\n",
      "Corresponding TXT file not found for 5b4eebbca3a7297352e1c34d--8216661c-ed05-4cb6-8b14-bf4b19d5889a.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--cdc02e5c-c777-480d-a71d-1f6bb5b4c0c0.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--932f3a09-aae6-4f6c-a775-36b87cfc99c4.pdf\n",
      "Corresponding TXT file not found for 5369998ea3a729239d2052a2--faa705a2-03d2-4a53-acb0-5d68ad2fa2f0.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--36e2f659-3ac3-4b48-97b2-77478db71599.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--a40c5b39-55a5-4275-8748-ef41b5e7ee79.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--079c0356-a11d-47ef-8576-a32349393b26.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--542b674f-68de-4587-bfc1-060390377398.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--a668362a-a94a-45e3-982c-058db9dbb162.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--9dade2c9-3ed2-4803-bb44-94fc06de7e8f.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--cb1c24ec-4910-46a4-ba09-e719e9dfbd13.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--edf4643b-c7eb-4932-9f7d-77210a4d4451.pdf\n",
      "Corresponding TXT file not found for 5ce2bd189ce2e77503079e2c--aad0478c-65b4-4f37-a20c-0514165e2d0d.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--59540b7b-9fc4-48b7-95c8-6e0015ba180e.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--60400be0-37e0-4b32-bc96-9d55daece02b.pdf\n",
      "Corresponding TXT file not found for 5cfa33d98b4c4110458b6223--539e3687-8058-4933-a981-36a2a2fafcc8.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--73ee1b3b-6e7c-4e64-a0b1-28dd84fcc84e.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--747bb864-0fe6-4b56-a7ef-226772a2096f.pdf\n",
      "Corresponding TXT file not found for 53699cefa3a729239d205adb--f630a1ab-c675-4497-ac5b-dd01fc4674da.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--63bd785a-7189-487d-8202-19f6931eac64.pdf\n",
      "Corresponding TXT file not found for 5bbcbf488b4c41041b850cf7--66a86a63-418c-4a54-835e-99f46f154bda.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--3598913d-58bb-4f89-8103-fac2ff58fa55.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--7a0371fd-d678-40da-a719-ca82300cefdc.pdf\n",
      "Corresponding TXT file not found for 53699cc6a3a729239d205a64--1ea6865b-73ab-404b-9648-51842fd5266f.pdf\n",
      "Corresponding TXT file not found for 5b4ee8d6a3a7296dbbe1c36c--000b4746-87e5-4c31-8118-25780d00ba0d.pdf\n",
      "Corresponding TXT file not found for 5bbc65c38b4c4164350acfaa--a33fb7e5-835d-482f-8b81-e2e031d922e1.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--6e530468-0019-4a60-80db-9c85f099fba7.pdf\n",
      "Corresponding TXT file not found for 5862299088ee3827d13f4e5d--f90cb7ff-f6ae-465b-aa9d-b7b5429c44d4.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--542340a8-bb1b-474f-9418-a7e1cb5ddc4e.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--1e3392b2-0138-4b76-bd06-7b3cc6df87a3.pdf\n",
      "Corresponding TXT file not found for 5369988aa3a729239d204fb5--c72943a7-8385-4e05-bd1c-de3000bf01c9.pdf\n",
      "Corresponding TXT file not found for 5ce2bd1c06e3e707af260cc9--1e9aebe2-3c90-41a4-863c-b4c2e339f004.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--126a895a-8022-4a43-bd1e-d9eccafd633a.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--b309fa7f-cc1b-4a28-90ce-a57bee60f35e.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--2f0395e9-b461-47f7-b32a-0241635a73b1.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--62a393ef-778d-43e2-a66d-42685ab9fabc.pdf\n",
      "Corresponding TXT file not found for 53699721a3a729239d204bf5--6f6ffb37-3aba-4281-9082-18963b981c86.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--7c43880f-4363-43d0-9b78-2c29de5f0dc9.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--130e1318-5eeb-457c-bd0d-983b3873dc43.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--31cd3099-574d-4f5e-864f-ec8d46b597f7.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--53276225-773d-4ceb-bb1c-b03777f2f1ca.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--7ab50887-4bf0-44f2-bb20-d103642380e2.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--ae79662d-3dc0-456a-9bd1-c46b14bd7b79.pdf\n",
      "Corresponding TXT file not found for 5b116d4588ee383e3b88f7d9--b0fbe3ad-f022-4b74-a5b8-8b91dbf60fc8.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--1b3a3acc-fbf4-46ef-853c-54c8d9eff935.pdf\n",
      "Corresponding TXT file not found for 53699e96a3a729239d205ef7--8a5bec8a-54a9-4ca8-88b3-08db906891d6.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--0773b367-07eb-4329-a3cd-1800008ba076.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--d2d405f7-d8d8-4b44-a7a3-e93f7e32f7ac.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--b91d46e0-b590-43c4-8afa-87110a3b2f6c.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--fd6d10d5-bef9-479c-b76a-db0e3894d5fa.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--7b6d88d5-4b39-42c8-a533-a30fb9624ab9.pdf\n",
      "Corresponding TXT file not found for 5a5f3b8988ee385e8e9e1c75--8ba43f3d-0a94-4b7a-b7ed-5df43640b0a0.pdf\n",
      "Corresponding TXT file not found for 5369998da3a729239d2052a0--27457d0d-294d-4a65-bba2-8de2e16ab52c.pdf\n",
      "Corresponding TXT file not found for 5bbcaf5e8b4c41668adc45a9--7382eb9d-3351-4fd5-af25-9c0546c5df09.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--987da031-263a-494b-b0ab-8ceedd6af627.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--40507075-8498-43db-8998-25a436709f5b.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--629f6224-0a3d-44e1-9ec0-63ff9840da50.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--dbffd297-1530-4023-80d3-681ab9ea0e32.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--229bc92f-cbc9-4e4a-9a56-407e6729f412.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--a1143c68-6f59-44a6-8939-55fed73bd795.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--8677cb6f-d4e8-4a28-abf6-85430ea39e8a.pdf\n",
      "Corresponding TXT file not found for 5e4b9dd78b4c417144e920b1--a5170aea-dbad-40f5-bc38-ffc72e8ac38e.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--ef5f5f65-3af9-4426-917a-d8a3fe3e6cbe.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--83f89e49-c20f-44f1-b65d-8fcd5755d52b.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--d3bfa269-deaa-4009-97bb-34843d1dcd84.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--a4edffcd-cddf-4c5a-8475-bc83c1c0f406.pdf\n",
      "Corresponding TXT file not found for 5cfa3b138b4c41274b725e0d--fe1639e1-5c92-428f-b465-cc558f05c804.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--23686809-6a4c-44d5-b47b-626c89581795.pdf\n",
      "Corresponding TXT file not found for 5bd1a835634f410210af123e--4e8419a2-e275-4e49-bfae-b2d20eabb31f.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--e572fd80-f226-43fb-afed-a598c0633a9d.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--4b2b108c-628f-4500-9d94-13d102804b76.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--df2e03c4-0a59-4ba2-b78e-fad23677383f.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--4116606c-6524-4c23-b393-fa082ea71327.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--4303988c-086d-4a48-949c-bbbb195b7a1d.pdf\n",
      "Corresponding TXT file not found for 540f9b52a3a72928898af4a9--b0c67cff-809d-4576-96e7-d61f4c2c6843.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--961dc06c-8046-4d5d-9dd0-05f6a8e2d125.pdf\n",
      "Corresponding TXT file not found for 5b6952a88b4c4119b83b67f7--13b84a92-b8e1-4a17-b160-edc046d30e56.pdf\n",
      "Corresponding TXT file not found for 5b4eeba4a3a7297352e1c348--c1ec77ca-76fb-49ba-87e1-593dfc9e325e.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--a158fcfa-a42b-42af-aba2-d3ae1480895c.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--d4b555ac-4b7e-49b3-aaeb-37a5a8ed6f7d.pdf\n",
      "Corresponding TXT file not found for 595c268da3a7296408d69b5e--dab3b66f-7bc0-47be-96ae-e97ec24044cb.pdf\n",
      "Corresponding TXT file not found for 5b10f5e2c751df2c87b90aae--ac1e38b8-febd-40d4-84e4-508641aabef6.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--ce710686-36f6-4b55-a62e-c65464908e3c.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--9396d265-9b1e-47b3-9f7f-557f85499e73.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--67959d58-145c-48be-8a30-b3a64501e66a.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--d3cb0e11-3054-47cf-85c7-11969025fce9.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--924de896-8d95-4447-b318-890e519039f9.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--0821210e-b384-4496-9456-411f75a15f13.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--ebacbda6-e40f-4d08-9c28-b2b8473ddb8c.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--130d62cf-e96a-4df6-83a8-ccb1e39b55e1.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--5e7b9d53-1a04-487b-924e-bd93e02c4060.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--6cb1d17e-06b5-4a34-9fb4-206e6e8a21d0.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--afb6de51-f61b-4237-a0f3-630655acd990.pdf\n",
      "Corresponding TXT file not found for 5b71a9ea8b4c414c31b9a5bc--abad77a3-ccb5-4f99-b09d-91d6fe8dcb68.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--1538624d-605a-4f2b-a1fd-dc89dc9a4903.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--80d4e30a-dbec-4cb7-b19f-762c2171b776.pdf\n",
      "Corresponding TXT file not found for 5cfa3c3f8b4c412b5014e725--b4550199-b182-49f0-8820-d58d1caedb2a.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--0b001c9e-1468-4e83-b8d1-22bc919e0eea.pdf\n",
      "Corresponding TXT file not found for 5b4ee817b5950877c5d496d8--9bfbe0a1-4744-4047-9283-c1969bd0def4.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--4cb23730-b354-4cbb-9d5d-c393a356f7f1.pdf\n",
      "Corresponding TXT file not found for 5bb6133b8b4c410ea4b9113c--529449f9-852f-4ea4-843f-683ee812ac06.pdf\n",
      "Corresponding TXT file not found for 5b116d4588ee383e3b88f7d9--f8cfb608-86ae-490e-bb89-8b845ac4c6ad.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--4bd096c5-a6fd-47fa-a6a0-5eb51f366098.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--bb04036c-f515-4f69-bdb3-d60a1e09ca7f.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--76a58868-b869-4625-b6b0-9a93ccdfccd4.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--879a06bd-88ba-4625-bfaa-f30c2fadbb57.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--58fa5d75-beec-498d-a244-a5e01031c9e1.pdf\n",
      "Corresponding TXT file not found for 5ce2bd1e06e3e707af260cd9--553450d9-db13-4cb2-ae6e-79b963a72085.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--c4d16830-d41d-49cc-9f91-93f785a65a80.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--3958194d-54f1-4aac-93c3-69a5bb485435.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--78a24e23-0abb-48bb-a752-a780d44b721c.pdf\n",
      "Corresponding TXT file not found for 5ce2bd199ce2e77507079e0d--2788dc90-07c7-405e-96b4-c462c036dd90.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--0531088d-de1d-4f81-aea5-6608db2d721c.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--47da7384-d962-4ae8-8a2b-4461daab49b0.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--e5fb2be0-deb9-400a-abae-186b79bb3d79.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--b31e617a-16d8-4d88-aab3-b7e5585d1b7c.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--f46264a3-78b2-40d6-a7e9-570a5b3b8b69.pdf\n",
      "Corresponding TXT file not found for 5dde52fc634f4162a84b1914--e864fb7a-a5bd-402e-b059-58009a7200a2.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--a5693d24-f502-4c1d-8f83-b30e1f3fc505.pdf\n",
      "Corresponding TXT file not found for 5e3ad64c8b4c41720a2526ad--9becadc6-0307-46e2-bb1b-c39ef9b60c10.pdf\n",
      "Corresponding TXT file not found for 53699cc6a3a729239d205a65--42a34773-7c86-4454-a6e9-cb9a5ee2b4e0.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--e76ec517-c84a-4988-8b1d-3a8d2cc9c7d2.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--5a0fb974-58c6-4b5a-875a-ce1eb11cc2fb.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--aeafcd86-3477-4944-8952-c6f9500c40fb.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--d77bf4db-a6bd-493d-84d3-da1ad356dde1.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--ca8ac719-220b-423a-a944-f22a16ca8642.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--aa3045c2-988f-417b-8c9b-fb83e2c4230c.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--9a3ca015-ab39-404c-8bec-f45da061a33a.pdf\n",
      "Corresponding TXT file not found for 5d13692f634f41364f8929b8--8edee5b4-acdf-456b-ad12-4b191a7eb0dc.pdf\n",
      "Corresponding TXT file not found for 54291fb888ee380329a59159--49434933-391b-4206-99ca-b905f6d682a7.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--3ce0b186-cdb3-4cf1-ac9f-c8cd6800eae7.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--f1b3ffac-caa5-4bc1-b754-ad45b66e3bbe.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--9218b56e-f2a9-4bcc-a865-939590a4c3c6.pdf\n",
      "Corresponding TXT file not found for 5b4ee8ccb595087918d496d3--15d607b6-9b0c-4b80-9e8c-151b791f60b5.pdf\n",
      "Corresponding TXT file not found for 5bd1a430634f417ec34dc24c--eae20898-04c9-47cd-8db4-b909babd4359.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--f4be00ce-249e-46fb-9226-491ea1b24bdf.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--455aeae7-2cbe-4aa9-beb9-e6182c977446.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--398aacf1-96ac-4953-8bc9-84a79e5bbf03.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--bdd6d093-ee8f-436c-b983-ddf55807a159.pdf\n",
      "Corresponding TXT file not found for 5b6952a88b4c4119b83b67f7--11a0bebf-69ec-4c4c-842d-fcfa139ad34c.pdf\n",
      "Corresponding TXT file not found for 5b71a9ea8b4c414c31b9a5bc--099ffe90-61ce-480e-afb2-0b7444ba217f.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--6a7eddb8-4b17-49fc-99cf-f72c35b698a5.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--7e9400c4-c06c-4a7b-b864-a3262e9fe970.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--f75787b7-ac56-4260-b51e-0cfd0c80202f.pdf\n",
      "Corresponding TXT file not found for 5c7929508b4c4150ff5452bc--0c2f82a2-6b7b-432a-b077-f0067bc69100.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--96e32fd1-0aad-4d1b-88f9-0420ced001cb.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--79fbf740-a48f-44fb-891d-fb51bfb974d4.pdf\n",
      "Corresponding TXT file not found for 5b4eeb8da3a7297352e1c343--916c3d98-08ee-4c93-8b02-d498d404c3d4.pdf\n",
      "Corresponding TXT file not found for 5bd054818b4c4113c9530a31--eddf3a8d-f5fe-4a72-8545-1631ce1ceb1c.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--ff0ddb11-d93b-4b33-91bb-a0f673f3e288.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--561b5f41-9630-464d-97f9-627083b79519.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--5a0cff1f-cede-4264-9f42-a916fd68aa02.pdf\n",
      "Corresponding TXT file not found for 5bbcaf5e8b4c41668adc45a9--6dc197a5-3cee-4fed-bcfb-dec12ccc6b4a.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--4bc1f108-2862-46ca-8f32-54cfc46d1b56.pdf\n",
      "Corresponding TXT file not found for 5de0626d9ce2e77584b62c09--eec6d3dd-b1e1-4be3-aad9-7fa9e8cb2184.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--5a3acc71-2ec0-479d-b126-30a6ced3eb78.pdf\n",
      "Corresponding TXT file not found for 5a83010d88ee38207d4f7d9c--1a1433d3-f91b-4f3f-b1e8-48250d0c9f8b.pdf\n",
      "Corresponding TXT file not found for 5b4ee8d6a3a7296dbbe1c36c--e0b27a88-669a-4538-a5f1-c1cabf800c7f.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--d8e640ce-98de-485f-bc7a-77fbe097a27c.pdf\n",
      "Corresponding TXT file not found for 5bbcbf488b4c41041b850cf7--3cf294cd-51dd-4755-b233-06b5ce668eb2.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--6575d37c-49f6-46fe-b47e-19045b56deb0.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--d0f16bd8-ecef-419d-8cb4-91dd884728a6.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--de03fbda-f7d8-4746-bc81-fd68eaa356b3.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--ca091411-6426-4c9d-8757-f2c2039a711f.pdf\n",
      "Corresponding TXT file not found for 5a83036388ee382283ed9fd8--1e203349-068f-4ba3-bfb3-569ea8f34278.pdf\n",
      "Corresponding TXT file not found for 5b5ec5c188ee3842dbe6ba19--23f7dd8b-afcb-4bc2-8980-4265d424545c.pdf\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.document import ConversionStatus\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "import evaluate\n",
    "\n",
    "# Directories for input files\n",
    "TEXT_FILES_DIR = '/home/pi/Téléchargements/data-gouv-pdf-txt/data_gouv_txt/'\n",
    "DOWNLOAD_DIR = '/home/pi/Téléchargements/data-gouv-pdf-txt/data_gouv_pdf/'\n",
    "\n",
    "# Initialize Hugging Face Evaluate metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Configurer Docling\n",
    "def create_docling_converter():\n",
    "    options = PdfPipelineOptions()\n",
    "    options.do_ocr = True\n",
    "    options.generate_page_images = False\n",
    "    converter = DocumentConverter(\n",
    "        allowed_formats=[InputFormat.PDF],\n",
    "        format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=options)}\n",
    "    )\n",
    "    return converter\n",
    "\n",
    "# Calcul des métriques avec Evaluate\n",
    "def calculate_metrics(pdf_text: str, txt_text: str) -> dict:\n",
    "    # CER\n",
    "    cer_score = cer_metric.compute(predictions=[pdf_text], references=[txt_text])\n",
    "    \n",
    "    # WER\n",
    "    wer_score = wer_metric.compute(predictions=[pdf_text], references=[txt_text])\n",
    "    \n",
    "    # ROUGE\n",
    "    rouge_scores = rouge_metric.compute(predictions=[pdf_text], references=[txt_text])\n",
    "    \n",
    "    # BLEU\n",
    "    bleu_score = bleu_metric.compute(predictions=[pdf_text.split()], references=[[txt_text.split()]])\n",
    "    \n",
    "    return {\n",
    "        \"CER\": cer_score['cer'],\n",
    "        \"WER\": wer_score['wer'],\n",
    "        \"ROUGE-1\": rouge_scores['rouge1'].fmeasure,\n",
    "        \"ROUGE-2\": rouge_scores['rouge2'].fmeasure,\n",
    "        \"ROUGE-L\": rouge_scores['rougeL'].fmeasure,\n",
    "        \"BLEU\": bleu_score['bleu']\n",
    "    }\n",
    "\n",
    "# Processus principal\n",
    "def process_files(pdf_dir: str, txt_dir: str) -> pd.DataFrame:\n",
    "    pdf_dir = Path(pdf_dir)\n",
    "    txt_dir = Path(txt_dir)\n",
    "\n",
    "    converter = create_docling_converter()\n",
    "    results = []\n",
    "\n",
    "    for pdf_file in pdf_dir.glob(\"*.pdf\"):\n",
    "        txt_file = txt_dir / f\"{pdf_file.stem}.txt\"\n",
    "\n",
    "        if not txt_file.exists():\n",
    "            print(f\"Corresponding TXT file not found for {pdf_file.name}\")\n",
    "            continue\n",
    "\n",
    "        # Extraire le texte du PDF\n",
    "        result = list(converter.convert_all([pdf_file]))[0]\n",
    "        if result.status != ConversionStatus.SUCCESS:\n",
    "            print(f\"Failed to process {pdf_file.name}\")\n",
    "            continue\n",
    "\n",
    "        pdf_text = result.document.text_content.strip()\n",
    "        with txt_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            txt_text = f.read().strip()\n",
    "\n",
    "        # Calculer les métriques\n",
    "        metrics = calculate_metrics(pdf_text, txt_text)\n",
    "        metrics.update({\n",
    "            \"PDF File\": pdf_file.name,\n",
    "            \"TXT File\": txt_file.name\n",
    "        })\n",
    "        results.append(metrics)\n",
    "\n",
    "    # Créer un DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Lancer le processus\n",
    "df_results = process_files(DOWNLOAD_DIR, TEXT_FILES_DIR)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(df_results.head())\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier CSV\n",
    "# output_csv = \"comparison_results_hf.csv\"\n",
    "# df_results.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "# print(f\"Results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeux de données pour un benchmark IDP (Intelligent Document Processing)\n",
    "\n",
    "Pour un benchmark orienté IDP (Intelligent Document Processing) avec un objectif métier clair, tel que la transformation de documents non structurés en informations exploitables, voici une sélection de petits jeux de données adaptés :\n",
    "\n",
    "## 1. FUNSD (Form Understanding in Noisy Scanned Documents)\n",
    "- **Description** : Un jeu de données contenant 199 formulaires annotés avec des entités liées à leur contenu (en-têtes, questions, réponses). Ces formulaires sont souvent utilisés pour tester la capacité des modèles à extraire et structurer des informations.\n",
    "- **Lien** : [FUNSD GitHub](https://github.com/microsoft/unilm/tree/master/funsd)\n",
    "- **Format** : PDF et annotations JSON.\n",
    "- **Avantages** : Conçu pour les tâches d'extraction de données clés et la structuration de documents semi-structurés.\n",
    "\n",
    "## 2. CORD (Consolidated Receipt Dataset)\n",
    "- **Description** : Un jeu de données qui comprend des tickets de caisse annotés pour la reconnaissance des entités (montants, articles, etc.).\n",
    "- **Lien** : [CORD GitHub](https://github.com/clovaai/cord)\n",
    "- **Format** : Images et JSON.\n",
    "- **Avantages** : Idéal pour tester l'extraction d'informations dans des documents commerciaux et structurés.\n",
    "\n",
    "## 3. Kleister Charity\n",
    "- **Description** : Un jeu de données conçu pour la classification de documents et l'extraction d'informations à partir de documents longs, notamment des contrats et des rapports.\n",
    "- **Lien** : [Kleister GitHub](https://github.com/allenai/kleister-charity)\n",
    "- **Format** : JSON et PDF.\n",
    "- **Avantages** : Bien adapté pour les cas d'usage métier où les données sont enfermées dans de longs rapports.\n",
    "\n",
    "## 4. SROIE (Scanned Receipts OCR and Information Extraction)\n",
    "- **Description** : Un jeu de données qui contient des scans de reçus avec annotations sur des champs spécifiques (nom de l'entreprise, date, montant, etc.).\n",
    "- **Lien** : [SROIE Resources](https://rrc.cvc.uab.es/?ch=13)\n",
    "- **Format** : Images et JSON.\n",
    "- **Avantages** : Permet de tester la capacité à traiter des documents bruités ou mal scannés.\n",
    "\n",
    "## 5. Invoice Dataset\n",
    "- **Description** : Un petit jeu de données contenant des factures annotées, conçu pour tester les capacités de reconnaissance des documents structurés.\n",
    "- **Lien** : [Invoice Dataset Kaggle](https://www.kaggle.com/datasets/ashishraut64/invoice-dataset)\n",
    "- **Format** : PDF et annotations.\n",
    "- **Avantages** : Couvre les besoins liés à l'extraction de données dans les cas d'usage financier et administratif.\n",
    "\n",
    "## 6. RVL-CDIP (Ryerson Vision Lab Complex Document Information Processing)\n",
    "- **Description** : Un jeu de données comprenant 400 000 pages de documents classées dans 16 catégories (lettres, mémos, bulletins, etc.).\n",
    "- **Lien** : [RVL-CDIP Details](http://www.cs.cmu.edu/~aharley/rvl-cdip/)\n",
    "- **Format** : TIFF et catégories.\n",
    "- **Avantages** : Utile pour tester la classification de documents non structurés.\n",
    "\n",
    "---\n",
    "\n",
    "## **Sélection Optimale**\n",
    "Pour un benchmark léger mais pertinent, il est recommandé de commencer par **FUNSD**, **CORD**, et **SROIE**, car ils couvrent des cas variés d'extraction de données :\n",
    "1. **Documents semi-structurés** (ex. formulaires).\n",
    "2. **Documents commerciaux structurés** (tickets, factures).\n",
    "3. **Documents bruités** (scans de qualité variable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléhargement et extraction des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, zipfile, shutil, re, requests, shutil\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Répertoire de téléchargement des datasets\n",
    "DATASET_DIR = Path(\"datasets\")\n",
    "DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dossier de sortie unique pour tous les fichiers\n",
    "MERGED_DATASET_DIR = Path(\"merged_dataset\")\n",
    "MERGED_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sources des datasets\n",
    "DATASET_SOURCES = {\n",
    "    \"FUNSD\": {\n",
    "        \"type\": \"url\",\n",
    "        \"url\": \"https://guillaumejaume.github.io/FUNSD/dataset.zip\",\n",
    "        \"zip_name\": \"funsd.zip\",\n",
    "        \"annotation_folder\": \"dataset/training_data/annotations\",\n",
    "        \"image_folder\": \"dataset/training_data/images\",\n",
    "    },\n",
    "    \"CORD\": {\n",
    "        \"type\": \"huggingface\",\n",
    "        \"huggingface_dataset\": \"naver-clova-ix/cord-v2\",\n",
    "    },\n",
    "    \"SROIE\": {\n",
    "        \"type\": \"kaggle\",\n",
    "        \"kaggle_dataset\": \"urbikn/sroie-datasetv2\",\n",
    "        \"zip_name\": \"sroie-datasetv2.zip\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(name, info, download_dir):\n",
    "    \"\"\"\n",
    "    Télécharge un dataset en fonction de son type et sauvegarde les fichiers associés.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Traitement du dataset : {name}\")\n",
    "\n",
    "    if info[\"type\"] == \"url\":\n",
    "        # Gestion pour les fichiers ZIP depuis une URL (FUNSD)\n",
    "        zip_path = download_dir / info[\"zip_name\"]\n",
    "        if not zip_path.exists():\n",
    "            print(f\"[INFO] {name} - Téléchargement depuis l'URL : {info['url']}\")\n",
    "            try:\n",
    "                with requests.get(info[\"url\"], stream=True) as response:\n",
    "                    response.raise_for_status()\n",
    "                    with open(zip_path, \"wb\") as f:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                print(f\"[SUCCESS] {name} - Fichier téléchargé : {zip_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {name} - Erreur lors du téléchargement : {e}\")\n",
    "        else:\n",
    "            print(f\"[INFO] {name} - Fichier déjà présent, téléchargement ignoré : {zip_path}\")\n",
    "\n",
    "    elif info[\"type\"] == \"huggingface\":\n",
    "        # Gestion pour CORD (Hugging Face)\n",
    "        dataset_dir = download_dir / name\n",
    "        annotations_file = dataset_dir / \"annotations.json\"\n",
    "        dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Téléchargement des images\n",
    "        print(f\"[INFO] {name} - Téléchargement des images depuis Hugging Face\")\n",
    "        try:\n",
    "            dataset = load_dataset(info[\"huggingface_dataset\"], split=\"train\")\n",
    "            annotations = []  # Stockage des annotations\n",
    "\n",
    "            for i, example in enumerate(dataset):\n",
    "                # Vérifie si l'image existe déjà\n",
    "                image_path = dataset_dir / f\"{name}_image_{i}.jpg\"\n",
    "                if not image_path.exists():\n",
    "                    if \"image\" in example and example[\"image\"] is not None:\n",
    "                        example[\"image\"].save(image_path)\n",
    "                        print(f\"[SUCCESS] {name} - Image sauvegardée : {image_path}\")\n",
    "\n",
    "                # Collecte les annotations\n",
    "                annotations.append({\n",
    "                    \"file_name\": f\"{name}_image_{i}.jpg\",\n",
    "                    \"annotations\": example.get(\"text\", \"\"),\n",
    "                    \"bbox\": example.get(\"bboxes\", []),\n",
    "                })\n",
    "\n",
    "            # Vérifie si les annotations existent déjà avant de les sauvegarder\n",
    "            if not annotations_file.exists():\n",
    "                with open(annotations_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(annotations, f, ensure_ascii=False, indent=4)\n",
    "                print(f\"[SUCCESS] {name} - Annotations sauvegardées : {annotations_file}\")\n",
    "            else:\n",
    "                print(f\"[INFO] {name} - Annotations déjà présentes, téléchargement ignoré : {annotations_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {name} - Erreur lors du téléchargement Hugging Face : {e}\")\n",
    "\n",
    "    elif info[\"type\"] == \"kaggle\":\n",
    "        # Gestion pour les fichiers ZIP depuis Kaggle (SROIE)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        zip_path = download_dir / info[\"zip_name\"]\n",
    "        if not zip_path.exists():\n",
    "            print(f\"[INFO] {name} - Téléchargement depuis Kaggle : {info['kaggle_dataset']}\")\n",
    "            try:\n",
    "                api.dataset_download_files(info[\"kaggle_dataset\"], path=download_dir, unzip=False)\n",
    "                print(f\"[SUCCESS] {name} - Fichier téléchargé : {zip_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {name} - Erreur lors du téléchargement Kaggle : {e}\")\n",
    "        else:\n",
    "            print(f\"[INFO] {name} - Fichier déjà présent, téléchargement ignoré : {zip_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"[ERROR] {name} - Type de dataset inconnu\")\n",
    "\n",
    "# Téléchargement des datasets\n",
    "for dataset_name, dataset_info in DATASET_SOURCES.items():\n",
    "    download_dataset(dataset_name, dataset_info, DATASET_DIR)\n",
    "\n",
    "print(\"[INFO] Téléchargement des datasets terminé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing et merge des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des conflits de noms\n",
    "def copy_without_overwrite(src, dst_dir):\n",
    "    dst_path = dst_dir / src.name\n",
    "    counter = 1\n",
    "    while dst_path.exists():\n",
    "        dst_path = dst_dir / f\"{src.stem}_{counter}{src.suffix}\"\n",
    "        counter += 1\n",
    "    shutil.copy(src, dst_path)\n",
    "    return dst_path.name\n",
    "\n",
    "# Fonction pour tenter de corriger automatiquement les erreurs JSON\n",
    "def fix_json_format(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Remplacer les single quotes par des double quotes pour les clés\n",
    "        fixed_content = re.sub(r\"(?<!\\\\)'([^']*?)'(?!:)\", r'\"\\1\"', content)\n",
    "\n",
    "        # Retirer les virgules finales après le dernier élément\n",
    "        fixed_content = re.sub(r\",(\\s*[\\}\\]])\", r\"\\1\", fixed_content)\n",
    "\n",
    "        # Écrire le contenu corrigé dans le même fichier\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(fixed_content)\n",
    "        print(f\"[INFO] Format JSON corrigé pour : {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERREUR] Impossible de corriger le fichier JSON : {file_path}. Erreur : {e}\")\n",
    "\n",
    "# Fonction pour charger des fichiers JSON avec validation\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[ERREUR] Fichier JSON invalide : {file_path}. Erreur : {e}\")\n",
    "        print(\"[INFO] Tentative de correction automatique...\")\n",
    "        fix_json_format(file_path)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"[ERREUR] Impossible de charger le fichier JSON après correction : {file_path}. Erreur : {e}\")\n",
    "            return None\n",
    "\n",
    "# Extraction et fusion pour FUNSD\n",
    "def process_funsd(funsd_dir, merged_dir):\n",
    "    print(\"[INFO] Traitement FUNSD...\")\n",
    "    annotations = []\n",
    "    for subset in [\"training_data\", \"testing_data\"]:\n",
    "        annotation_dir = funsd_dir / \"dataset\" / subset / \"annotations\"\n",
    "        image_dir = funsd_dir / \"dataset\" / subset / \"images\"\n",
    "        \n",
    "        for annotation_file in tqdm(annotation_dir.glob(\"*.json\"), desc=f\"[FUNSD] {subset}\"):\n",
    "            data = load_json_file(annotation_file)\n",
    "            if data:\n",
    "                image_file = image_dir / f\"{annotation_file.stem}.png\"\n",
    "                if image_file.exists():\n",
    "                    new_image_name = copy_without_overwrite(image_file, merged_dir)\n",
    "                    text = \" \".join([block[\"text\"] for block in data.get(\"form\", [])])\n",
    "                    annotations.append({\"file_name\": new_image_name, \"text\": text})\n",
    "    return annotations\n",
    "\n",
    "# Extraction et fusion pour SROIE\n",
    "def process_sroie(sroie_dir, merged_dir):\n",
    "    print(\"[INFO] Traitement SROIE...\")\n",
    "    annotations = []\n",
    "    for subset in [\"train\", \"test\"]:\n",
    "        annotation_dir = sroie_dir / \"SROIE2019\" / subset / \"entities\"\n",
    "        image_dir = sroie_dir / \"SROIE2019\" / subset / \"img\"\n",
    "\n",
    "        for annotation_file in tqdm(annotation_dir.glob(\"*.txt\"), desc=f\"[SROIE] {subset}\"):\n",
    "            with open(annotation_file, \"r\") as f:\n",
    "                text = f.read().strip()\n",
    "            image_file = image_dir / f\"{annotation_file.stem}.jpg\"\n",
    "            if image_file.exists():\n",
    "                new_image_name = copy_without_overwrite(image_file, merged_dir)\n",
    "                annotations.append({\"file_name\": new_image_name, \"text\": text})\n",
    "    return annotations\n",
    "\n",
    "# Extraction et fusion pour CORD\n",
    "def process_cord(cord_dir, merged_dir):\n",
    "    print(\"[INFO] Traitement CORD...\")\n",
    "    annotations = []\n",
    "    annotation_file = cord_dir / \"annotations.json\"\n",
    "    data = load_json_file(annotation_file)\n",
    "    if data:\n",
    "        for item in tqdm(data, desc=\"[CORD]\"):\n",
    "            image_file = cord_dir / item[\"file_name\"]\n",
    "            if image_file.exists():\n",
    "                new_image_name = copy_without_overwrite(image_file, merged_dir)\n",
    "                annotations.append({\n",
    "                    \"file_name\": new_image_name,\n",
    "                    \"text\": item.get(\"text\", \"\"),\n",
    "                    \"bboxes\": item.get(\"bboxes\", [])\n",
    "                })\n",
    "    return annotations\n",
    "\n",
    "# Fusionner tous les datasets\n",
    "def merge_datasets():\n",
    "    all_annotations = []\n",
    "    # Traiter FUNSD\n",
    "    funsd_dir = DATASET_DIR / \"funsd\"\n",
    "    all_annotations.extend(process_funsd(funsd_dir, MERGED_DATASET_DIR))\n",
    "\n",
    "    # Traiter SROIE\n",
    "    sroie_dir = DATASET_DIR / \"sroie-datasetv2\"\n",
    "    all_annotations.extend(process_sroie(sroie_dir, MERGED_DATASET_DIR))\n",
    "\n",
    "    # Traiter CORD\n",
    "    cord_dir = DATASET_DIR / \"CORD\"\n",
    "    all_annotations.extend(process_cord(cord_dir, MERGED_DATASET_DIR))\n",
    "\n",
    "    # Sauvegarder les annotations fusionnées\n",
    "    merged_annotations_file = MERGED_DATASET_DIR / \"merged_annotations.json\"\n",
    "    with open(merged_annotations_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_annotations, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"[INFO] Annotations fusionnées sauvegardées dans : {merged_annotations_file}\")\n",
    "\n",
    "# Exécution principale\n",
    "if __name__ == \"__main__\":\n",
    "    merge_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark et evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import pytesseract\n",
    "import layoutparser as lp\n",
    "from paddleocr import PaddleOCR\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "import evaluate\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions, TableStructureOptions, TesseractCliOcrOptions, TableFormerMode\n",
    ")\n",
    "from docling.datamodel.document import ConversionStatus\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "# Configuration\n",
    "hv.extension(\"bokeh\")\n",
    "pytesseract.pytesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# Initialize Hugging Face Evaluate metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Generate interactive plots\n",
    "def generate_interactive_plots(results_df):\n",
    "    time_plot = hv.Bars(results_df, kdims=[\"Document\", \"Modèle\"], vdims=[\"Temps (s)\"]).opts(\n",
    "        title=\"Temps de traitement par modèle\",\n",
    "        xlabel=\"Document et Modèle\",\n",
    "        ylabel=\"Temps (s)\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "        tools=[\"hover\"]\n",
    "    )\n",
    "    bleu_plot = hv.Bars(results_df, kdims=[\"Document\", \"Modèle\"], vdims=[\"BLEU\"]).opts(\n",
    "        title=\"Score BLEU par modèle\",\n",
    "        xlabel=\"Document et Modèle\",\n",
    "        ylabel=\"BLEU\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "        tools=[\"hover\"]\n",
    "    )\n",
    "    rouge_plot = hv.Bars(results_df, kdims=[\"Document\", \"Modèle\"], vdims=[\"ROUGE\"]).opts(\n",
    "        title=\"Score ROUGE par modèle\",\n",
    "        xlabel=\"Document et Modèle\",\n",
    "        ylabel=\"ROUGE\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "        tools=[\"hover\"]\n",
    "    )\n",
    "    cer_plot = hv.Bars(results_df, kdims=[\"Document\", \"Modèle\"], vdims=[\"CER\"]).opts(\n",
    "        title=\"Score CER par modèle\",\n",
    "        xlabel=\"Document et Modèle\",\n",
    "        ylabel=\"CER\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "        tools=[\"hover\"]\n",
    "    )\n",
    "    wer_plot = hv.Bars(results_df, kdims=[\"Document\", \"Modèle\"], vdims=[\"WER\"]).opts(\n",
    "        title=\"Score WER par modèle\",\n",
    "        xlabel=\"Document et Modèle\",\n",
    "        ylabel=\"WER\",\n",
    "        width=800,\n",
    "        height=400,\n",
    "        tools=[\"hover\"]\n",
    "    )\n",
    "    return time_plot, bleu_plot, rouge_plot, cer_plot, wer_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Docling\n",
    "def initialize_docling():\n",
    "    # Configuration du pipeline Docling\n",
    "    pipeline_options = PdfPipelineOptions(\n",
    "        do_table_structure=True,\n",
    "        do_ocr=True,\n",
    "        table_structure_options=TableStructureOptions(\n",
    "            do_cell_matching=True, mode=TableFormerMode.ACCURATE\n",
    "        ),\n",
    "        ocr_options=TesseractCliOcrOptions(\n",
    "            lang=[\"eng\", \"fra\"], tesseract_cmd=\"tesseract\"\n",
    "        ),\n",
    "        generate_page_images=True,\n",
    "        generate_picture_images=True,\n",
    "        generate_table_images=True,\n",
    "    )\n",
    "\n",
    "    # Retourne un convertisseur Docling configuré\n",
    "    return DocumentConverter(\n",
    "        allowed_formats=[InputFormat.PDF],\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        },\n",
    "    )\n",
    "\n",
    "# Initialisation de Docling\n",
    "docling_converter = initialize_docling()\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "paddle_ocr = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)\n",
    "\n",
    "# Utility functions\n",
    "def calculate_similarity(text1, text2):\n",
    "    return SequenceMatcher(None, text1, text2).ratio()\n",
    "\n",
    "def process_with_layoutparser(doc_path):\n",
    "    try:\n",
    "        if doc_path.suffix.lower() == \".pdf\":\n",
    "            images = convert_from_path(str(doc_path))\n",
    "            image = np.array(images[0])\n",
    "        else:\n",
    "            image = cv2.imread(str(doc_path))\n",
    "        \n",
    "        ocr_agent = lp.TesseractAgent(languages=\"eng\")\n",
    "        layout = ocr_agent.detect(image)\n",
    "\n",
    "        # Vérification et extraction robuste des textes\n",
    "        return \" \".join([block.text if hasattr(block, \"text\") else str(block) for block in layout])\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec LayoutParser : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def process_with_tesseract(doc_path):\n",
    "    try:\n",
    "        image = convert_from_path(str(doc_path))[0] if doc_path.suffix.lower() == \".pdf\" else cv2.imread(str(doc_path))\n",
    "        return pytesseract.image_to_string(np.array(image))\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec Tesseract : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_with_docling(doc_path):\n",
    "    if not docling_converter:\n",
    "        print(\"Docling non initialisé.\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        result = docling_converter.convert(doc_path)\n",
    "        if result.status == ConversionStatus.SUCCESS and result.document:\n",
    "            return result.document.export_to_text()\n",
    "        elif result.status == ConversionStatus.PARTIAL_SUCCESS and result.document:\n",
    "            return result.document.export_to_text()\n",
    "        print(f\"Docling : Conversion échouée pour {doc_path}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec Docling : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_with_paddleocr(doc_path):\n",
    "    try:\n",
    "        image = convert_from_path(str(doc_path))[0] if doc_path.suffix.lower() == \".pdf\" else cv2.imread(str(doc_path))\n",
    "        image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)\n",
    "        result = paddle_ocr.ocr(image, cls=True)\n",
    "        return \" \".join([line[1][0] for line in result[0]])\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec PaddleOCR : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def evaluate_metrics(pred_text, ref_text):\n",
    "    if not pred_text.strip() or not ref_text.strip():\n",
    "        print(\"Attention : Texte prédit ou de référence vide.\")\n",
    "        return None, None, None, None\n",
    "    try:\n",
    "        bleu = bleu_metric.compute(predictions=[pred_text], references=[[ref_text]])[\"bleu\"]\n",
    "        rouge = rouge_metric.compute(predictions=[pred_text], references=[ref_text])[\"rougeL\"]\n",
    "        cer = cer_metric.compute(predictions=[pred_text], references=[ref_text])\n",
    "        wer = wer_metric.compute(predictions=[pred_text], references=[ref_text])\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation des métriques : {e}\")\n",
    "        bleu, rouge, cer, wer = None, None, None, None\n",
    "    return bleu, rouge, cer, wer\n",
    "\n",
    "# Benchmark des modèles\n",
    "def benchmark_models(files, references):\n",
    "    results = []\n",
    "    for file_path in files:\n",
    "        reference_text = references.get(file_path.name, \"\")\n",
    "        for process_fn, model_name in [\n",
    "            (process_with_layoutparser, \"LayoutParser\"),\n",
    "            (process_with_tesseract, \"Tesseract\"),\n",
    "            (process_with_docling, \"Docling\"),\n",
    "            (process_with_paddleocr, \"PaddleOCR\"),\n",
    "        ]:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                pred_text = process_fn(file_path)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                metrics = evaluate_metrics(pred_text, reference_text)\n",
    "            except Exception as e:\n",
    "                elapsed_time, metrics = None, (None, None, None, None)\n",
    "                print(f\"Erreur avec {model_name} : {e}\")\n",
    "            results.append({\n",
    "                \"Document\": file_path.name,\n",
    "                \"Modèle\": model_name,\n",
    "                \"Temps (s)\": elapsed_time,\n",
    "                \"BLEU\": metrics[0],\n",
    "                \"ROUGE\": metrics[1],\n",
    "                \"CER\": metrics[2],\n",
    "                \"WER\": metrics[3],\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Interactive dashboard\n",
    "def create_dashboard():\n",
    "    dataset_slider = pn.widgets.IntSlider(name=\"Taille du Dataset\", start=10, end=50, step=10)\n",
    "    benchmark_btn = pn.widgets.Button(name=\"Lancer le Benchmark\", button_type=\"primary\")\n",
    "    error_pane = pn.pane.Markdown(\"\", styles={\"color\": \"red\"})\n",
    "    result_table = pn.pane.DataFrame(sizing_mode=\"stretch_width\")\n",
    "    plot_panes = [pn.pane.HoloViews() for _ in range(5)]\n",
    "\n",
    "    def run_benchmark(event):\n",
    "        error_pane.object = \"\"\n",
    "        merged_dataset_dir = Path(\"merged_dataset\")\n",
    "        annotations_file = merged_dataset_dir / \"merged_annotations.json\"\n",
    "        if not annotations_file.exists():\n",
    "            error_pane.object = f\"**Erreur :** {annotations_file} est introuvable.\"\n",
    "            return\n",
    "\n",
    "        with open(annotations_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            references = {item[\"file_name\"]: item[\"text\"] for item in json.load(f)}\n",
    "\n",
    "        files = list(merged_dataset_dir.glob(\"*\"))\n",
    "        valid_files = [file for file in files if file.suffix.lower() in [\".pdf\", \".png\", \".jpg\"]]\n",
    "        selected_files = valid_files[:dataset_slider.value]\n",
    "        if not selected_files:\n",
    "            error_pane.object = \"**Erreur :** Aucun fichier valide trouvé.\"\n",
    "            return\n",
    "\n",
    "        results_df = benchmark_models(selected_files, references)\n",
    "        result_table.object = results_df\n",
    "\n",
    "        plots = generate_interactive_plots(results_df)\n",
    "        for plot_pane, plot in zip(plot_panes, plots):\n",
    "            plot_pane.object = plot\n",
    "\n",
    "    benchmark_btn.on_click(run_benchmark)\n",
    "\n",
    "    return pn.Column(\n",
    "        dataset_slider,\n",
    "        benchmark_btn,\n",
    "        error_pane,\n",
    "        result_table,\n",
    "        *plot_panes\n",
    "    )\n",
    "\n",
    "pn.extension()\n",
    "create_dashboard().servable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import pytesseract\n",
    "import layoutparser as lp\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "import evaluate\n",
    "\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions, TableStructureOptions, TesseractCliOcrOptions, TableFormerMode\n",
    ")\n",
    "from docling.datamodel.document import ConversionStatus\n",
    "\n",
    "# Configuration\n",
    "hv.extension(\"bokeh\")\n",
    "pytesseract.pytesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# Initialize Metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Initialize Docling\n",
    "def initialize_docling():\n",
    "    options = PdfPipelineOptions(\n",
    "        do_table_structure=True,\n",
    "        do_ocr=True,\n",
    "        table_structure_options=TableStructureOptions(do_cell_matching=True, mode=TableFormerMode.ACCURATE),\n",
    "        ocr_options=TesseractCliOcrOptions(lang=[\"eng\", \"fra\"], tesseract_cmd=\"tesseract\"),\n",
    "        generate_page_images=True,\n",
    "        generate_picture_images=True,\n",
    "        generate_table_images=True,\n",
    "    )\n",
    "    return DocumentConverter(\n",
    "        allowed_formats=[\"pdf\"],\n",
    "        format_options={\"pdf\": PdfFormatOption(pipeline_options=options, backend=PyPdfiumDocumentBackend)}\n",
    "    )\n",
    "\n",
    "docling_converter = initialize_docling()\n",
    "\n",
    "# Calcul de similarité\n",
    "def calculate_similarity(text1, text2):\n",
    "    return SequenceMatcher(None, text1, text2).ratio()\n",
    "\n",
    "\n",
    "# Initialize Docling\n",
    "def initialize_docling():\n",
    "    pipeline_options = PdfFormatOption(\n",
    "        pipeline_options={\"do_ocr\": True},\n",
    "        backend=PyPdfiumDocumentBackend\n",
    "    )\n",
    "    return DocumentConverter(allowed_formats=[\"pdf\"], format_options={\"pdf\": pipeline_options})\n",
    "\n",
    "docling_converter = initialize_docling()\n",
    "\n",
    "# Traitement avec LayoutParser\n",
    "def process_with_layoutparser(doc_path):\n",
    "    try:\n",
    "        if doc_path.suffix.lower() == \".pdf\":\n",
    "            images = convert_from_path(str(doc_path))\n",
    "            image = np.array(images[0])\n",
    "        else:\n",
    "            image = cv2.imread(str(doc_path))\n",
    "        \n",
    "        ocr_agent = lp.TesseractAgent(languages=\"eng\")\n",
    "        layout = ocr_agent.detect(image)\n",
    "\n",
    "        # Vérification et extraction robuste des textes\n",
    "        return \" \".join([block.text if hasattr(block, \"text\") else str(block) for block in layout])\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec LayoutParser : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Traitement avec Tesseract\n",
    "def process_with_tesseract(doc_path):\n",
    "    if doc_path.suffix.lower() == \".pdf\":\n",
    "        images = convert_from_path(str(doc_path))\n",
    "        image = np.array(images[0])\n",
    "    else:\n",
    "        image = cv2.imread(str(doc_path))\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "def process_with_docling(doc_path):\n",
    "    try:\n",
    "        result = docling_converter.convert(doc_path)\n",
    "        if result.status == ConversionStatus.SUCCESS and result.document:\n",
    "            return result.document.export_to_text()\n",
    "        elif result.status == ConversionStatus.PARTIAL_SUCCESS:\n",
    "            return result.document.export_to_text() if result.document else \"\"\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec Docling : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def evaluate_metrics(pred_text, ref_text):\n",
    "    if not pred_text.strip() or not ref_text.strip():\n",
    "        print(\"Attention : Texte prédit ou de référence vide.\")\n",
    "        return None, None, None, None\n",
    "    try:\n",
    "        bleu = bleu_metric.compute(predictions=[pred_text], references=[[ref_text]])[\"bleu\"]\n",
    "        rouge = rouge_metric.compute(predictions=[pred_text], references=[ref_text])[\"rougeL\"]\n",
    "        cer = cer_metric.compute(predictions=[pred_text], references=[ref_text])\n",
    "        wer = wer_metric.compute(predictions=[pred_text], references=[ref_text])\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation des métriques : {e}\")\n",
    "        bleu, rouge, cer, wer = None, None, None, None\n",
    "    return bleu, rouge, cer, wer\n",
    "\n",
    "# Benchmark des modèles\n",
    "def benchmark_models(files, references):\n",
    "    results = []\n",
    "    for file_path in files:\n",
    "        reference_text = references.get(file_path.name, \"\")\n",
    "        for process_fn, model_name in [\n",
    "            (process_with_layoutparser, \"LayoutParser\"),\n",
    "            (process_with_tesseract, \"Tesseract\"),\n",
    "            (process_with_docling, \"Docling\"),\n",
    "        ]:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                pred_text = process_fn(file_path)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                metrics = evaluate_metrics(pred_text, reference_text)\n",
    "            except Exception as e:\n",
    "                elapsed_time, metrics = None, (None, None, None, None)\n",
    "                print(f\"Erreur avec {model_name} : {e}\")\n",
    "            results.append({\n",
    "                \"Document\": file_path.name,\n",
    "                \"Modèle\": model_name,\n",
    "                \"Temps (s)\": elapsed_time,\n",
    "                \"BLEU\": metrics[0],\n",
    "                \"ROUGE\": metrics[1],\n",
    "                \"CER\": metrics[2],\n",
    "                \"WER\": metrics[3],\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Tableau de bord interactif\n",
    "def create_dashboard():\n",
    "    dataset_slider = pn.widgets.IntSlider(name=\"Taille du Dataset\", start=10, end=50, step=10)\n",
    "    benchmark_btn = pn.widgets.Button(name=\"Lancer le Benchmark\", button_type=\"primary\")\n",
    "    error_pane = pn.pane.Markdown(\"\", styles={\"color\": \"red\"})\n",
    "    result_table = pn.pane.DataFrame(sizing_mode=\"stretch_width\")\n",
    "    plot_panes = [pn.pane.HoloViews() for _ in range(5)]\n",
    "\n",
    "    def run_benchmark(event):\n",
    "        error_pane.object = \"\"\n",
    "        merged_dataset_dir = Path(MERGED_DATASET_DIR)\n",
    "        annotations_file = MERGED_DATASET_DIR / \"merged_annotations.json\"\n",
    "        if not annotations_file.exists():\n",
    "            error_pane.object = f\"**Erreur :** {annotations_file} est introuvable.\"\n",
    "            return\n",
    "\n",
    "        with open(annotations_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            references = {item[\"file_name\"]: item[\"text\"] for item in json.load(f)}\n",
    "\n",
    "        files = list(merged_dataset_dir.glob(\"*\"))\n",
    "        valid_files = [file for file in files if file.suffix.lower() in [\".pdf\", \".png\", \".jpg\"]]\n",
    "        selected_files = valid_files[:dataset_slider.value]\n",
    "        if not selected_files:\n",
    "            error_pane.object = \"**Erreur :** Aucun fichier valide trouvé.\"\n",
    "            return\n",
    "\n",
    "        results_df = benchmark_models(selected_files, references)\n",
    "        result_table.object = results_df\n",
    "\n",
    "        plots = generate_interactive_plots(results_df)\n",
    "        for plot_pane, plot in zip(plot_panes, plots):\n",
    "            plot_pane.object = plot\n",
    "\n",
    "    benchmark_btn.on_click(run_benchmark)\n",
    "\n",
    "    return pn.Column(\n",
    "        dataset_slider,\n",
    "        benchmark_btn,\n",
    "        error_pane,\n",
    "        result_table,\n",
    "        *plot_panes\n",
    "    )\n",
    "\n",
    "pn.extension()\n",
    "create_dashboard().servable()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gliner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
